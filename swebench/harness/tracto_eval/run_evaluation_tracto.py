# This file contains logic for running evaluations on TractoAI: <https://tracto.ai/>.

from __future__ import annotations

import json
import yt.wrapper as yt
from typing import Iterable
import datetime
import dataclasses

from pathlib import Path
from swebench.harness.docker_build import setup_logger
from swebench.harness.reporting import make_run_report
from swebench.harness.utils import EvaluationError
from typing import cast, Any


from swebench.harness.constants import (
    APPLY_PATCH_FAIL,
    APPLY_PATCH_PASS,
    RUN_EVALUATION_LOG_DIR,
    SWEbenchInstance,
)
from swebench.harness.grading import get_eval_report
from swebench.harness.test_spec.test_spec import make_test_spec, TestSpec


@dataclasses.dataclass
class TestInput:
    test_spec: TestSpec
    prediction: dict
    run_id: str
    timeout: int


@yt.yt_dataclass
class TestOutput:
    instance_id: str


# @yt.yt_dataclass
class TestOutput:
    instance_id: str
    test_output: str
    report_json_str: str
    run_instance_log: str
    patch_diff: str
    log_dir: Path
    errored: bool


def get_log_dir(pred: dict, run_id: str, instance_id: str) -> Path:
    model_name_or_path = cast(
        str, pred.get("model_name_or_path", "None").replace("/", "__")
    )
    return RUN_EVALUATION_LOG_DIR / run_id / model_name_or_path / instance_id


class RunInstanceTracto:
    def __call__(self, test_input_raw: dict) -> Iterable[TestOutput]:
        test_input = TestInput(**test_input_raw)
        print(f"[instance_id={test_input.test_spec.instance_id}]")
        yield TestOutput(instance_id=test_input.test_spec.instance_id)


def run_instances_tracto(
    predictions: dict,
    instances: list[SWEbenchInstance],
    full_dataset: list[SWEbenchInstance],
    run_id: str,
    timeout: int,
):
    """
    Run all instances for the given predictions on Modal.

    Args:
        predictions (dict): Predictions dict generated by the model
        instances (list): List of instances
        run_id (str): Run ID
        timeout (int): Timeout for running tests
    """
    test_specs = [make_test_spec(instance) for instance in instances]

    run_test_specs: list[TestSpec] = []

    # steps:
    # 1. upload source table
    # 2. run map, save results to output table
    # 3. parse output table

    # Check for instances that have already been run
    for test_spec in test_specs:
        log_dir = get_log_dir(
            predictions[test_spec.instance_id], run_id, test_spec.instance_id
        )
        if log_dir.exists():
            continue
        run_test_specs.append(test_spec)

    if run_test_specs:
        run_dir = (
            "//home/llm/sbkarasik/tracto-swe-bench/"
            f"{run_id}-{datetime.datetime.now().isoformat()}"
        )
        print(f"{run_dir=}")

        yt.create("map_node", run_dir, recursive=True)

        source_table_path = f"{run_dir}/source"
        output_table_path = f"{run_dir}/output"
        print(f"{source_table_path=}")
        source_table_rows = [
            TestInput(
                test_spec=dataclasses.asdict(test_spec),
                prediction=predictions[test_spec.instance_id],
                run_id=run_id,
                timeout=timeout,
            )
            for test_spec in run_test_specs
        ]
        yt.write_table(
            table=source_table_path,
            # row_type=TestInput,
            input_stream=[dataclasses.asdict(row) for row in source_table_rows],
        )

        yt.run_map(
            RunInstanceTracto(),
            source_table_path,
            output_table_path,
            spec={
                "mapper": {
                    "docker_image": "cr.turing.yt.nebius.yt/home/llm/sbkarasik/registry/swebench-fork:dev",
                }
            },
            # output_format=TestOutput,
        )

    #     for result in results:
    #         result = cast(TestOutput, result)

    #         # Save logs locally
    #         log_dir = result.log_dir
    #         log_dir.mkdir(parents=True, exist_ok=True)
    #         with open(log_dir / "run_instance.log", "w") as f:
    #             f.write(result.run_instance_log)
    #         with open(log_dir / "test_output.txt", "w") as f:
    #             f.write(result.test_output)
    #         with open(log_dir / "patch.diff", "w") as f:
    #             f.write(result.patch_diff)
    #         with open(log_dir / "report.json", "w") as f:
    #             try:
    #                 report_json = json.loads(result.report_json_str)
    #                 json.dump(report_json, f, indent=4)
    #             except Exception:
    #                 # This happens if the test fails with any exception
    #                 print(f"{result.instance_id}: no report.json")

    # make_run_report(predictions, full_dataset, run_id)


def main(
    dataset_name: str,
    split: str,
    instance_ids: list,
    predictions_path: str,
    max_workers: int,
    force_rebuild: bool,
    cache_level: str,
    clean: bool,
    open_file_limit: int,
    run_id: str,
    timeout: int,
    namespace: str | None,
    rewrite_reports: bool,
    modal: bool,
    # tracto: bool,
    instance_image_tag: str = "latest",
    report_dir: str = ".",
):
    """
    Run evaluation harness for the given dataset and predictions.
    """
    from swebench.harness.constants import (
        KEY_INSTANCE_ID,
    )

    from swebench.harness.utils import (
        load_swebench_dataset,
        get_predictions_from_file,
    )

    from swebench.harness.run_evaluation import get_dataset_from_preds

    namespace = None if namespace == "" else namespace

    # load predictions as map of instance_id to prediction
    predictions = get_predictions_from_file(predictions_path, dataset_name, split)
    predictions = {pred[KEY_INSTANCE_ID]: pred for pred in predictions}

    # get dataset from predictions
    dataset = get_dataset_from_preds(
        dataset_name, split, instance_ids, predictions, run_id, rewrite_reports
    )
    full_dataset = load_swebench_dataset(dataset_name, split, instance_ids)

    run_instances_tracto(predictions, dataset, full_dataset, run_id, timeout)


if __name__ == "__main__":
    import argparse

    from swebench.harness.utils import str2bool

    parser = argparse.ArgumentParser(
        description="Run evaluation harness for the given dataset and predictions.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Common args
    parser.add_argument(
        "--dataset_name",
        default="SWE-bench/SWE-bench_Lite",
        type=str,
        help="Name of dataset or path to JSON file.",
    )
    parser.add_argument(
        "--split", type=str, default="test", help="Split of the dataset"
    )
    parser.add_argument(
        "--instance_ids",
        nargs="+",
        type=str,
        help="Instance IDs to run (space separated)",
    )
    parser.add_argument(
        "--predictions_path",
        type=str,
        help="Path to predictions file - if 'gold', uses gold predictions",
        required=True,
    )

    # Local execution args
    parser.add_argument(
        "--max_workers",
        type=int,
        default=4,
        help="Maximum number of workers (should be <= 75%% of CPU cores)",
    )
    parser.add_argument(
        "--open_file_limit", type=int, default=4096, help="Open file limit"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=1_800,
        help="Timeout (in seconds) for running tests for each instance",
    )
    parser.add_argument(
        "--force_rebuild",
        type=str2bool,
        default=False,
        help="Force rebuild of all images",
    )
    parser.add_argument(
        "--cache_level",
        type=str,
        choices=["none", "base", "env", "instance"],
        help="Cache level - remove images above this level",
        default="env",
    )
    # if clean is true then we remove all images that are above the cache level
    # if clean is false, we only remove images above the cache level if they don't already exist
    parser.add_argument(
        "--clean", type=str2bool, default=False, help="Clean images above cache level"
    )
    parser.add_argument(
        "--run_id", type=str, required=True, help="Run ID - identifies the run"
    )
    parser.add_argument(
        "--namespace", type=str, default="swebench", help="Namespace for images"
    )
    parser.add_argument(
        "--instance_image_tag", type=str, default="latest", help="Instance image tag"
    )
    parser.add_argument(
        "--rewrite_reports",
        type=str2bool,
        default=False,
        help="Doesn't run new instances, only writes reports for instances with existing test outputs",
    )
    parser.add_argument(
        "--report_dir", type=str, default=".", help="Directory to write reports to"
    )

    # Modal execution args
    parser.add_argument("--modal", type=str2bool, default=False, help="Run on Modal")

    # Traco execution args
    # parser.add_argument("--tracto", type=str2bool, default=False, help="Run on Tracto")

    args = parser.parse_args()
    main(**vars(args))
